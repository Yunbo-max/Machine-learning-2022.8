{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o68JSms-mzE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Shannon and Differential Entropy\n",
        "# Shannon entropy for a binary random variable (e.g., coin flip) is:\n",
        "\n",
        "def binary_entropy(my_p):\n",
        "    return (my_p-1)*np.log(1-my_p) - my_p*np.log(my_p)\n",
        "\n",
        "p = np.linspace(0.001, 0.999, 1000)\n",
        "\n",
        "H = binary_entropy(p)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.title('Shannon entropy of Bernoulli trial')\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('H (nats)')\n",
        "ax.plot(p,H)\n",
        "\n",
        "\n",
        "# Cross-Entropy\n",
        "# Cross-entropy is a concept derived from KL divergence. Its detail is beyond the scope of this series except to mention that it provides us with the cross-entropy cost function.\n",
        "def cross_entropy(y, a):\n",
        "    return -1*(y*np.log(a) + (1-y)*np.log(1-a))\n",
        "\n",
        "cross_entropy(1, 0.3)"
      ]
    }
  ]
}