{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNIcNHZ392Zq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from math import factorial\n",
        "\n",
        "# random distribution + plot\n",
        "\n",
        "# ns = np.array([2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096])\n",
        "# np.random.seed(42) # for reproducibility\n",
        "# np.random.rand(3,4)\n",
        "# np.random.randn(3,4)\n",
        "# np.random.binomial(1, 0.5)\n",
        "# heads_count = [np.random.binomial(n, 0.5) for n in ns]\n",
        "# proportion_heads = heads_count/ns\n",
        "#\n",
        "#\n",
        "# fig, ax = plt.subplots()\n",
        "# plt.xlabel('Number of coin flips in experiment')\n",
        "# plt.ylabel('Proportion of flips that are heads')\n",
        "# plt.axhline(0.5, color='orange')\n",
        "# ax.scatter(ns, proportion_heads)\n",
        "# np.cov(x, y, ddof=0)\n",
        "# st.pearsonr(x, y)[0]\n",
        "# Covariance and correlation only account for linear relationships. Two variables could be non-linearly related to each other and these metrics could come out as zero\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# statistics\n",
        "# n_experiments = 1000\n",
        "# heads_count = np.random.binomial(5, 0.5, n_experiments)\n",
        "#\n",
        "# heads, event_count = np.unique(heads_count, return_counts=True)\n",
        "# event_proba = event_count/n_experiments\n",
        "#\n",
        "# plt.bar(heads, event_proba, color='mediumpurple')\n",
        "# plt.xlabel('Heads flips (out of 5 tosses)')\n",
        "# plt.ylabel('Event probability')\n",
        "#\n",
        "# plt.show()\n",
        "\n",
        "# distribution\n",
        "# x_array = np.arange(-5, 5, 0.03)\n",
        "# y_pdf = st.norm.pdf(x_array,0,1)\n",
        "# y_pdf = st.skewnorm.pdf(x_array,0,1)\n",
        "# plt.plot(x_array, y_pdf)\n",
        "# plt.show()\n",
        "#\n",
        "# z_score_stat, p_value = st.normaltest(y_pdf)\n",
        "# print(z_score_stat,p_value)\n",
        "\n",
        "\n",
        "\n",
        "# def coinflip_prob(n, k):\n",
        "#     n_choose_k = factorial(n)/(factorial(k)*factorial(n-k))\n",
        "#     return n_choose_k/2**n\n",
        "#\n",
        "# P = [coinflip_prob(5, x) for x in range(6)]\n",
        "# E = sum([P[x]*x for x in range(6)])\n",
        "#\n",
        "# count = [1,2,3,4,5,6,7,7]\n",
        "# np.mean(count)\n",
        "# np.var(x)\n",
        "# np.std(x)\n",
        "# np.median(count)\n",
        "# st.sem(x)\n",
        "# print(st.mode(count)[0][0])\n",
        "\n",
        "#\n",
        "x = st.skewnorm.rvs(0, size=1000)\n",
        "q = np.percentile(x, [25, 50, 75])\n",
        "np.quantile(x, [.95, .99])\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "sns.boxplot(x=x)\n",
        "plt.hist(x, color = 'lightgray')\n",
        "print(q)\n",
        "plt.show()\n",
        "# Box edges define the inter-quartile range (IQR):\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from math import factorial\n",
        "\n",
        "u = np.random.uniform(size=10000)\n",
        "sns.displot(u)\n",
        "\n",
        "x = np.random.normal(size=10000)\n",
        "sns.displot(x, kde=True)\n",
        "\n",
        "\n",
        "def sample_mean_calculator(input_dist, sample_size, n_samples):\n",
        "    sample_means = []\n",
        "    for i in range(n_samples):\n",
        "        sample = np.random.choice(input_dist, size=sample_size, replace=False)\n",
        "        sample_means.append(sample.mean())\n",
        "    return sample_means\n",
        "sns.displot(sample_mean_calculator(x, 100, 1000), color='green', kde=True)\n",
        "\n",
        "\n",
        "s = st.skewnorm.rvs(10, size=10000)\n",
        "sns.displot(sample_mean_calculator(s, 10, 1000), color='green', kde=True)\n",
        "\n",
        "\n",
        "# Sampling from a multimodal distribution\n",
        "# The Central Limit Theorem\n",
        "m = np.concatenate((np.random.normal(size=5000), np.random.normal(loc = 4.0, size=5000)))\n",
        "sns.displot(sample_mean_calculator(m, 1000, 1000), color='green', kde=True)\n",
        "\n",
        "# Log-Normal Distribution\n",
        "# Its logarithm has a skewed distribution:\n",
        "x = np.random.lognormal(size=10000) # defaults to standard normal mu=0, sigma=1\n",
        "\n",
        "# Laplace Distribution\n",
        "x = np.random.laplace(size=10000)\n",
        "sns.displot(x)\n",
        "sns.displot(x, kde=True)\n",
        "\n",
        "# discrete distribution\n",
        "# Binomial Distribution\n",
        "n = 5\n",
        "n_experiments = 1000\n",
        "heads_count = np.random.binomial(n, 0.5, n_experiments)\n",
        "heads, event_count = np.unique(heads_count, return_counts=True)\n",
        "event_proba = event_count/n_experiments\n",
        "plt.bar(heads, event_proba, color='mediumpurple')\n",
        "plt.xlabel('Heads flips (out of 5 tosses)')\n",
        "_ = plt.ylabel('Event probability')\n",
        "\n",
        "# Poisson Distribution\n",
        "lam=5\n",
        "n=1000\n",
        "samples = np.random.poisson(lam, n)\n",
        "samples[0:20]\n",
        "x, x_count = np.unique(samples, return_counts=True)\n",
        "Px = x_count/n\n",
        "plt.bar(x, Px, color='mediumpurple')\n",
        "plt.title('PMF of Poisson with lambda = {}'.format(lam))\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('P(x)')\n",
        "\n",
        "# Gaussian mixture model (GMM) is common type of mixture distribution, wherein all of the component distributions are normal.\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------\n",
        "# statistics\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "x_i = 85\n",
        "\n",
        "mu = 90\n",
        "sigma = 2\n",
        "y = np.random.normal(mu, sigma, 10000)\n",
        "sns.displot(y, color='gray')\n",
        "plt.axvline(mu, color='orange')\n",
        "for v in [-3, -2, -1, 1, 2, 3]:\n",
        "    plt.axvline(mu+v*sigma, color='olivedrab')\n",
        "    plt.axvline(x_i, color='purple')\n",
        "\n",
        "z = (x_i - np.mean(y))/np.std(y)\n",
        "\n",
        "print(100*len(np.where(y > 85)[0])/10000)\n",
        "print(np.percentile(y, 1))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# limited number of data and for one data\n",
        "# z = (85 - mu)/xigma\n",
        "len(np.where(y > 85)[0])\n",
        "\n",
        "# seems like infinity of data\n",
        "st.norm.ppf(.025)\n",
        "st.norm.ppf(.975)\n",
        "# +-1.959963984540054 xigma  a/2=5%/2 = 2.5%\n",
        "p_below = st.norm.cdf(-2.5)\n",
        "p_above = 1-st.norm.cdf(2.5)\n",
        "# +-2.5xigma ---> concrete number in the graph\n",
        "p_outside = p_below + p_above\n",
        "# p-values\n",
        "\n",
        "\n",
        "# Where z-scores apply to individual values only, t-tests enables us to compare (the mean of) a sample of multiple values to a reference mean.\n",
        "# single-sample t-test(1 samples)\n",
        "x = [48, 50, 54, 60]\n",
        "xbar = np.mean(x)\n",
        "sx = st.sem(x)\n",
        "t = (xbar-50)/sx\n",
        "st.ttest_1samp(x, 50)\n",
        "# Ttest_1sampResult(statistic=1.1338934190276817, pvalue=0.3392540508564543)\n",
        "\n",
        "# Welch's Independent t-test (2samples)\n",
        "sf = f.var(ddof=1)\n",
        "sm = m.var(ddof=1)\n",
        "nf = f.size\n",
        "nm = m.size\n",
        "t = (fbar-mbar)/(sf/nf + sm/nm)**(1/2)\n",
        "# p value\n",
        "st.ttest_ind(f, m, equal_var=False)\n",
        "\n",
        "# Student's Paired t-test\n",
        "# Indeed, with an independent t-test we could even have different sample sizes in the two groups whereas this is impossible with a paired t-test.\n",
        "# t = (dbar-0)/sd(statistic=3.3541019662496847), pvalue=0.02846020325433834\n",
        "st.ttest_rel(min15, min1)\n",
        "\n",
        "\n",
        "\n",
        "# Machine Learning Examples\n",
        "# Single-sample: Does my stochastic model tend to be more accurate than an established benchmark?\n",
        "# Independent samples: Does my model have unwanted bias in it, e.g., do white men score higher than other demographic groups with HR model?\n",
        "# Paired samples: Is new TensorFlow.js model significantly faster? (paired by browser / device)\n",
        "\n",
        "\n",
        "# Confidence Intervals\n",
        "def CIerr_calc(my_z, my_s, my_n):\n",
        "    return my_z*(my_s/my_n**(1/2))\n",
        "xbar = x.mean()\n",
        "s = x.std()\n",
        "n = x.size\n",
        "CIerr = CIerr_calc(z, s, n)\n",
        "xbar + CIerr\n",
        "xbar - CIerr\n",
        "\n",
        "\n",
        "# ANOVA: Analysis of Variance , enables us to compare more than two samples\n",
        "# To apply ANOVA, we must make three assumptions:\n",
        "# Independent samples\n",
        "# Normally-distributed populations\n",
        "# Homoscedasticity: Population standard deviations are equal\n",
        "st.f_oneway(t, b, d)\n",
        "# F_onewayResult(statistic=0.22627752438542714, pvalue=0.7980777848719299)\n",
        "\n",
        "\n",
        "\n",
        "#Pearson Correlation Coefficient\n",
        "\n",
        "# covariance provides a measure of how related the variables are to each other:\n",
        "product = []\n",
        "for i in range(n):\n",
        "    product.append((x[i]-xbar)*(y[i]-ybar))\n",
        "cov = sum(product)/n\n",
        "# Correlation\n",
        "r = cov/(np.std(x)*np.std(y))\n",
        "\n",
        "# correlation + p value\n",
        "st.pearsonr(x, y)\n",
        "# The Coefficient of Determination\n",
        "st.pearsonr(iris.sepal_length, iris.sepal_width)[0]**2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In brief, three criteria are required for inferring causal relationships:\n",
        "# Covariation: Two variables vary together (this criterion is satisfied by sepal and petal length)\n",
        "# Temporal precedence: The affected variable must vary after the causal variable is varied.\n",
        "# Elimination of extraneous variables: We must be sure no third variable is causing the variation. This can be tricky for data we obtained through observation alone, but easier when we can control the causal variable, e.g., with (ideally double-blind) randomized control trials.\n",
        "\n",
        "\n",
        "\n",
        "# Bonferroni correction\n",
        "# If you perform 20 statistical tests where there is no real effect (i.e., the null hypothesis is true), then we would expect one of them to come up significant by chance alone (i.e., a false positive or Type I error).\n",
        "# If you perform a hundred tests in such a circumstance, then you should expect five false positives.\n",
        "a = 0.05, a/n = 0.05/10 =0.005\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------\n",
        "# Regression\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Linear Least Squares for Fitting a Line to Points on a Cartesian Plane\n",
        "#  fitting a line to points on a Cartesian plane (2-D surface, with -axis perpendicular to horizontal -axis). To fit such a line, the only parameters we require are a -intercept (say, ) and a slope (say, ):\n",
        "\n",
        "# beta1 = cov/np.var(x)\n",
        "# beta0 = ybar - beta1*xbar\n",
        "# xline = np.linspace(4, 8, 1000)\n",
        "# yline = beta0 + beta1*xline\n",
        "#\n",
        "# sns.scatterplot(x=x, y=y)\n",
        "# plt.plot(xline, yline, color='orange')\n",
        "\n",
        "\n",
        "\n",
        "x = np.array([0, 1, 2, 3, 4, 5, 6, 7.])\n",
        "y = np.array([1.86, 1.31, .62, .33, .09, -.67, -1.23, -1.37])\n",
        "\n",
        "cov_mat = np.cov(x, y)\n",
        "beta1 = cov_mat[0,1]/cov_mat[0,0]\n",
        "beta0 = y.mean() - beta1*x.mean()\n",
        "xline = np.linspace(0, 7, 1000)\n",
        "yline = beta0 + beta1*xline\n",
        "sns.scatterplot(x=x, y=y)\n",
        "x_i = 4.5\n",
        "y_i = beta0 + beta1*x_i\n",
        "plt.title(\"Clinical Trial\")\n",
        "plt.xlabel(\"Drug dosage (mL)\")\n",
        "plt.ylabel(\"Forgetfulness\")\n",
        "plt.plot(xline, yline, color='orange')\n",
        "plt.scatter(x_i, y_i, marker='o', color='purple')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Ordinary Least Squares\n",
        "# estimate the parameters of regression models that have more than one predictor variable\n",
        "x = np.array([1, 2, 3, 4.])\n",
        "y = np.array([6, 5, 7, 10.])\n",
        "A = np.array([[8, 20],[20, 60]])\n",
        "z = np.array([56, 154])\n",
        "Ainv = np.linalg.inv(A)\n",
        "w = np.dot(Ainv, z)\n",
        "xline = np.linspace(1, 4, 1000)\n",
        "yline = w[0] + w[1]*xline\n",
        "fig, ax = plt.subplots()\n",
        "plt.title('Generative Adversarial Network')\n",
        "plt.xlabel('Number of convolutional layers')\n",
        "plt.ylabel('Image realism (out of 10)')\n",
        "ax.scatter(x, y)\n",
        "plt.plot(xline, yline, color='orange')\n",
        "\n",
        "# ncidentally, residuals are the distances between\n",
        "X = np.concatenate([np.matrix(np.ones(x.size)).T, np.matrix(x).T], axis=1)\n",
        "yhat = np.dot(X, w)\n",
        "fig, ax = plt.subplots()\n",
        "plt.title('Generative Adversarial Network')\n",
        "plt.xlabel('Number of convolutional layers')\n",
        "plt.ylabel('Image realism (out of 10)')\n",
        "ax.scatter(x, y)\n",
        "plt.plot(xline, yline, color='orange')\n",
        "for i in range(x.size):\n",
        "    plt.plot([x[i],x[i]], [y[i],yhat[0,i]], color='darkred')\n",
        "plt.show()\n",
        "\n",
        "# The above OLS approach expands to a wide variety of circumstances:\n",
        "#\n",
        "# Multiple features (, the predictors)\n",
        "# Polynomial (typically quadratic) features, e.g.,\n",
        "# Interacting features, e.g.,\n",
        "# Discrete, categorical features, incl. any combination of continuous and discrete features\n",
        "\n",
        "import statsmodels.api as sm\n",
        "sns.scatterplot(x='sepal_length', y='petal_length', hue='species', data=iris)\n",
        "dummy = pd.get_dummies(iris.species)\n",
        "y = iris.petal_length\n",
        "X = pd.concat([iris.sepal_length, dummy.setosa, dummy.versicolor], axis=1)\n",
        "X = sm.add_constant(X)\n",
        "model = sm.OLS(y, X)\n",
        "result = model.fit()\n",
        "result.summary()\n",
        "beta = result.params\n",
        "xline = np.linspace(4, 8, 1000)\n",
        "vi_yline = beta[0] + beta[1]*xline\n",
        "se_yline = beta[0] + beta[1]*xline + beta[2]\n",
        "ve_yline = beta[0] + beta[1]*xline + beta[3]\n",
        "sns.scatterplot(x='sepal_length', y='petal_length', hue='species', data=iris)\n",
        "plt.plot(xline, vi_yline, color='darkgreen')\n",
        "plt.plot(xline, se_yline, color='darkblue')\n",
        "_ = plt.plot(xline, ve_yline, color='orange')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Logistic Regression----- a binary outcome\n",
        "\n",
        "gender = pd.get_dummies(titanic['sex'])\n",
        "clas = pd.get_dummies(titanic['class'])\n",
        "y = titanic.survived\n",
        "X = pd.concat([clas.First, clas.Second, gender.female, titanic.age], axis=1)\n",
        "X = sm.add_constant(X)\n",
        "model = sm.Logit(y, X, missing='drop') # some rows contain NaN\n",
        "result = model.fit()\n",
        "result.summary()"
      ]
    }
  ]
}