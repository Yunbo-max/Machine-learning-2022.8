{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zMFYV9tB8Dz"
      },
      "outputs": [],
      "source": [
        "# Cost Functions\n",
        "# mean absolute error (MAE)\n",
        "# other cost functions out there (e.g., cross-entropy cost is the typical choice for a deep learning classifier),\n",
        "\n",
        "# Segment 1: The Machine Learning Approach to Optimization\n",
        "# The Statistical Approach to Regression: Ordinary Least Squares\n",
        "# When Statistical Approaches to Optimization Break Down\n",
        "# The Machine Learning Solution\n",
        "\n",
        "\n",
        "# Segment 2: Gradient Descent\n",
        "# Objective Functions\n",
        "# Cost / Loss / Error Functions\n",
        "# Minimizing Cost with Gradient Descent\n",
        "# Learning Rate\n",
        "# Critical Points, incl. Saddle Points\n",
        "# Gradient Descent from Scratch with PyTorch\n",
        "# The Global Minimum and Local Minima\n",
        "# Mini-Batches and Stochastic Gradient Descent (SGD)\n",
        "# Learning Rate Scheduling\n",
        "# Maximizing Reward with Gradient Ascent\n",
        "\n",
        "\n",
        "\n",
        "# Segment 3: Fancy Deep Learning Optimizers\n",
        "# A Layer of Artificial Neurons in PyTorch\n",
        "# Jacobian Matrices\n",
        "# Hessian Matrices and Second-Order Optimization\n",
        "# Momentum\n",
        "# Nesterov Momentum\n",
        "# AdaGrad\n",
        "# AdaDelta\n",
        "# RMSProp\n",
        "# Adam\n",
        "# Nadam\n",
        "# Training a Deep Neural Net\n",
        "# Resources for Further Study"
      ]
    }
  ]
}